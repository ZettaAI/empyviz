{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99168710-2c74-4b10-b987-278a8385e180",
   "metadata": {},
   "source": [
    "## Combine Nuclear Centroid Files\n",
    "\n",
    "This script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94a25c4-441f-49b3-b0c6-4b4b79582421",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from cloudfiles import CloudFiles\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e83abd-4c65-482e-9659-a7d356f40512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input path may be either a local file path, or a gs: (Google Storage) link\n",
    "input_path = \"gs://zheng_mouse_hippocampus_scratch_30/nuclei/com/seg/20240519214242\"\n",
    "# Output path currently must be a local file path\n",
    "output_path = \"~/zheng-mouse-hippo/nucleus-centroids.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce13eb-edec-4f51-901d-4e2c53b371ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "com_list = []\n",
    "if input_path.startswith(\"gs:\"):\n",
    "    from cloudfiles import CloudFiles\n",
    "    cf = CloudFiles(input_path)\n",
    "    files = cf.get(list(cf))\n",
    "    com_list = []\n",
    "    for f in files:\n",
    "        print(f\"\\n{f['path']}\", end=\"\")\n",
    "        com_list.append(np.frombuffer(f[\"content\"], dtype=np.int64).reshape([-1, 5]))\n",
    "    print('\\n')\n",
    "else:\n",
    "    for fn in os.listdir(output_path):\n",
    "        fp = os.path.join(output_path, fn)\n",
    "        with open(fp, \"rb\") as f:\n",
    "            print(f\"\\r{fn}\", end=\"\")\n",
    "            com_list.append(np.frombuffer(f.read(), dtype=np.int64).reshape([-1, 5]))\n",
    "centroids = np.concatenate(com_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a870cebb-32a7-494f-9e2b-4d0f37007d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=120)\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc28ef0-fbc6-47f1-ba54-17131180d70a",
   "metadata": {},
   "source": [
    "The data above is a segment (nucleus) ID, which may include multiple lines (in the case of a nucleus split by a chunk or otherwise oversegmented); and then a sum of X, Y, and Z values, and a voxel count (mass).  To collapse this down into a single centroid for each nucleus, we need to group by the ID, sum across rows in each group, and then divide by the mass (total voxel count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ee064c-61d9-452d-9897-032c51b70337",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(centroids[:, 1:], index=centroids[:, 0], columns=[\"x\", \"y\", \"z\", \"m\"])\n",
    "df = df.groupby(df.index).sum()\n",
    "df64 = df[[\"x\", \"y\", \"z\"]].div(df[\"m\"], axis=0).round().astype(int)\n",
    "df64[\"m\"] = df[\"m\"]\n",
    "df64.to_csv(output_path, index_label=\"id\")\n",
    "df64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29622c25-e090-4e0c-a540-cea9676a30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of \"m\" (total volume) values\n",
    "axes = df['m'].hist(bins=30)  # You can adjust the number of bins as needed\n",
    "axes.set_yscale('log')\n",
    "plt.title('Distribution of Nucleus Volume')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085b66e-ff20-4bb3-b2ba-88586fc64c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as above, but using log binning\n",
    "min_value = df['m'].min()\n",
    "max_value = df['m'].max()\n",
    "bins = np.logspace(np.log10(min_value), np.log10(max_value), num=30)  # Adjust the number of bins as needed\n",
    "df['m'].hist(bins=bins)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.title('Distribution of Nucleus Volume')\n",
    "plt.xlabel('Volume')\n",
    "plt.ylabel('Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11ea04-3f7b-4190-917d-f84e431a7bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a threshold based on the above, and filter out all the small junk.\n",
    "threshold = 60000\n",
    "\n",
    "df_filtered = df64[df64['m'] >= threshold]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4293920-8875-4633-961b-b9a351e90a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(output_path[:-4] + \"-filtered.csv\", index_label=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd998896-eca0-4468-a2c1-5704dd7ac613",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
